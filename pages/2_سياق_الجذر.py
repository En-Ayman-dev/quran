# pages/2_Ø³ÙŠØ§Ù‚_Ø§Ù„Ø¬Ø°Ø±.py
# -*- coding: utf-8 -*-

import re
import pandas as pd
import streamlit as st

# =======================
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø«Ø§Ø¨ØªØ© (Ù…Ø®ÙÙŠØ© Ø¹Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…)
# =======================
CSV_PATH = "quran_corrected_global.csv"
CL1_PATH = "Cl1.xlsx"

# Ø£Ø¹Ù…Ø¯Ø© CSV
COL_TASH = "3"        # Ø§Ù„Ø¢ÙŠØ© Ø¨Ø§Ù„ØªØ´ÙƒÙŠÙ„
COL_PLAIN = "4"       # Ø§Ù„Ø¢ÙŠØ© Ø¨Ø¯ÙˆÙ† ØªØ´ÙƒÙŠÙ„
COL_SURAH_NO = "1"    # Ø±Ù‚Ù… Ø§Ù„Ø³ÙˆØ±Ø©
COL_AYAH_NO = "2"     # Ø±Ù‚Ù… Ø§Ù„Ø¢ÙŠØ©
COL_SURAH_T = "10"    # Ø§Ø³Ù… Ø§Ù„Ø³ÙˆØ±Ø© Ø¨Ø§Ù„ØªØ´ÙƒÙŠÙ„
COL_SURAH_P = "11"    # Ø§Ø³Ù… Ø§Ù„Ø³ÙˆØ±Ø© Ø¨Ø¯ÙˆÙ† ØªØ´ÙƒÙŠÙ„

# Ø£Ø¹Ù…Ø¯Ø© Cl1.xlsx
CL1_WORD_COL = "Ø§Ù„ÙƒÙ„Ù…Ø©"
CL1_ROOT_COL = "Ø§Ù„Ø¬Ø°Ø±"

# =======================
# Ø£Ø¯ÙˆØ§Øª Ù„ØºÙˆÙŠØ©
# =======================
ARABIC_DIACRITICS_RE = re.compile(r"[\u0610-\u061A\u064B-\u065F\u0670\u06D6-\u06ED]")
TOKEN_RE = re.compile(r"[^\s]+")

def normalize_arabic(text: str) -> str:
    if not isinstance(text, str):
        return ""
    t = ARABIC_DIACRITICS_RE.sub("", text)
    t = t.replace("Ø£", "Ø§").replace("Ø¥", "Ø§").replace("Ø¢", "Ø§")
    t = t.replace("Ù±", "Ø§")
    t = t.replace("Ø©", "Ù‡")
    t = t.replace("Ù‰", "ÙŠ")
    t = t.replace("Ø¤", "Ùˆ").replace("Ø¦", "ÙŠ")
    t = re.sub(r"\s+", " ", t).strip()
    return t

def tokenize(text: str):
    if not isinstance(text, str) or not text:
        return []
    return TOKEN_RE.findall(text)

def safe_int(x):
    try:
        return int(str(x))
    except Exception:
        return 10**9

# =======================
# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# =======================
@st.cache_data(show_spinner=False)
def load_quran_csv(path: str) -> pd.DataFrame:
    try:
        return pd.read_csv(path, dtype=str, encoding="utf-8")
    except UnicodeDecodeError:
        return pd.read_csv(path, dtype=str, encoding="utf-8-sig")

@st.cache_data(show_spinner=False)
def load_xlsx(path: str) -> pd.DataFrame:
    return pd.read_excel(path, dtype=str)

@st.cache_data(show_spinner=False)
def build_root_maps(lex_df: pd.DataFrame):
    exact_map = {}
    norm_map = {}

    df = lex_df[[CL1_WORD_COL, CL1_ROOT_COL]].dropna().copy()
    df[CL1_WORD_COL] = df[CL1_WORD_COL].astype(str).str.strip()
    df[CL1_ROOT_COL] = df[CL1_ROOT_COL].astype(str).str.strip()

    for w, r in zip(df[CL1_WORD_COL], df[CL1_ROOT_COL]):
        exact_map[w] = r
        nw = normalize_arabic(w)
        if nw and nw not in norm_map:
            norm_map[nw] = r

    return exact_map, norm_map

@st.cache_data(show_spinner=True)
def index_ayah_roots(df: pd.DataFrame, exact_map: dict, norm_map: dict):
    roots_sets = []
    for _, row in df.iterrows():
        text_t = str(row.get(COL_TASH, "") or "")
        text_p = str(row.get(COL_PLAIN, "") or "")

        roots = set()

        for tok in tokenize(text_t):
            if tok in exact_map:
                roots.add(exact_map[tok])
            else:
                nt = normalize_arabic(tok)
                if nt in norm_map:
                    roots.add(norm_map[nt])

        for tok in tokenize(text_p):
            nt = normalize_arabic(tok)
            if nt in norm_map:
                roots.add(norm_map[nt])

        roots_sets.append(roots)

    return roots_sets

def format_surah_title(display_mode: str, sur_t: str, sur_p: str) -> str:
    if display_mode == "Ø¨Ø§Ù„ØªØ´ÙƒÙŠÙ„":
        return sur_t
    if display_mode == "Ø¨Ø¯ÙˆÙ† ØªØ´ÙƒÙŠÙ„":
        return sur_p
    return f"{sur_t} / {sur_p}"

def pick_text(display_mode: str, t: str, p: str) -> str:
    if display_mode == "Ø¨Ø§Ù„ØªØ´ÙƒÙŠÙ„":
        return t
    if display_mode == "Ø¨Ø¯ÙˆÙ† ØªØ´ÙƒÙŠÙ„":
        return p
    return f"{t}\n{p}"

# =======================
# ÙˆØ§Ø¬Ù‡Ø© Streamlit
# =======================
st.set_page_config(page_title="Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¬Ø°Ø±", page_icon="ğŸ§©", layout="wide")
st.title("ğŸ§© Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¬Ø°Ø± (Ù‚Ø¨Ù„ / Ø§Ù„Ø¢ÙŠØ© / Ø¨Ø¹Ø¯)")

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¹Ø±Ø¶ (Ù…ÙÙŠØ¯Ø© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…)
c_set1, c_set2, c_set3 = st.columns(3)
with c_set1:
    display_mode = st.radio("Ø¹Ø±Ø¶ Ø§Ù„Ù†Øµ", ["Ø¨Ø§Ù„ØªØ´ÙƒÙŠÙ„", "Ø¨Ø¯ÙˆÙ† ØªØ´ÙƒÙŠÙ„", "ÙƒÙ„Ø§Ù‡Ù…Ø§"], index=2)
with c_set2:
    prev_n = st.number_input("Ø¹Ø¯Ø¯ Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©", min_value=0, max_value=50, value=3)
with c_set3:
    next_n = st.number_input("Ø¹Ø¯Ø¯ Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„Ù„Ø§Ø­Ù‚Ø©", min_value=0, max_value=50, value=3)

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª (Ø¨Ø¯ÙˆÙ† ØªØ¯Ø®Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…)
try:
    quran_df_raw = load_quran_csv(CSV_PATH)
    lex_df_raw = load_xlsx(CL1_PATH)
except Exception as e:
    st.error(f"ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:\n{e}")
    st.stop()

# ÙØ­Øµ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø­Ø±Ø¬Ø©
required_cols = [COL_TASH, COL_PLAIN, COL_SURAH_NO, COL_AYAH_NO, COL_SURAH_T, COL_SURAH_P]
if not all(c in quran_df_raw.columns for c in required_cols):
    st.error("Ù…Ù„Ù CSV Ù„Ø§ ÙŠØ·Ø§Ø¨Ù‚ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©. Ø±Ø§Ø¬Ø¹ Ø§Ù„Ø«ÙˆØ§Ø¨Øª Ø£Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„Ù.")
    st.stop()

# Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù‚ÙˆØ§Ù…ÙŠØ³ + ÙÙ‡Ø±Ø³Ø© Ø§Ù„Ø¬Ø°ÙˆØ±
exact_map, norm_map = build_root_maps(lex_df_raw)

quran_df = quran_df_raw.copy()
quran_df["_roots_set"] = index_ayah_roots(quran_df, exact_map, norm_map)

# =======================
# Ø§Ù„Ø¨Ø­Ø«
# =======================
c1, c2 = st.columns([3, 1])
with c1:
    root_query = st.text_input("Ø£Ø¯Ø®Ù„ Ø§Ù„Ø¬Ø°Ø±", placeholder="Ù…Ø«Ø§Ù„: Ø®Ø±Ø±")
with c2:
    run = st.button("ğŸ” Ø¨Ø­Ø«", type="primary", use_container_width=True)

if run or root_query.strip():
    rq = root_query.strip()
    if not rq:
        st.stop()

    rq_norm = normalize_arabic(rq)

    def has_root(rs):
        return rq_norm in {normalize_arabic(x) for x in rs}

    hits = quran_df[quran_df["_roots_set"].apply(has_root)].copy()

    hits["_s"] = hits[COL_SURAH_NO].map(safe_int)
    hits["_a"] = hits[COL_AYAH_NO].map(safe_int)
    hits = hits.sort_values(["_s", "_a"]).reset_index(drop=True)

    total = len(hits)
    st.subheader("Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
    st.write(f"Ø¹Ø¯Ø¯ Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù„Ø¬Ø°Ø± **{rq}**: **{total}**")

    if total == 0:
        st.stop()

    # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…ØµØ­Ù Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„Ø³ÙŠØ§Ù‚
    q_all = quran_df.copy()
    q_all["_s"] = q_all[COL_SURAH_NO].map(safe_int)
    q_all["_a"] = q_all[COL_AYAH_NO].map(safe_int)
    q_all = q_all.sort_values(["_s", "_a"]).reset_index(drop=True)

    index_map = {
        (str(r[COL_SURAH_NO]), str(r[COL_AYAH_NO])): i
        for i, r in q_all.iterrows()
    }

    ctx_rows = []

    for _, hit in hits.iterrows():
        key = (str(hit[COL_SURAH_NO]), str(hit[COL_AYAH_NO]))
        idx = index_map.get(key)
        if idx is None:
            continue

        start = max(0, idx - prev_n)
        end = min(len(q_all) - 1, idx + next_n)

        sur_title = format_surah_title(
            display_mode,
            hit[COL_SURAH_T],
            hit[COL_SURAH_P]
        )

        before, after = [], []

        for j in range(start, idx):
            r = q_all.iloc[j]
            before.append(f"({r[COL_SURAH_NO]}:{r[COL_AYAH_NO]}) {pick_text(display_mode, r[COL_TASH], r[COL_PLAIN])}")

        center = pick_text(display_mode, hit[COL_TASH], hit[COL_PLAIN])

        for j in range(idx + 1, end + 1):
            r = q_all.iloc[j]
            after.append(f"({r[COL_SURAH_NO]}:{r[COL_AYAH_NO]}) {pick_text(display_mode, r[COL_TASH], r[COL_PLAIN])}")

        st.markdown(f"### [{hit[COL_SURAH_NO]}:{hit[COL_AYAH_NO]}] {sur_title}")
        st.markdown("**Ù‚Ø¨Ù„:**" if before else "**Ù‚Ø¨Ù„:** Ù„Ø§ ÙŠÙˆØ¬Ø¯")
        for b in before:
            st.markdown(f"- {b}")

        st.markdown("**Ø§Ù„Ø¢ÙŠØ© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©:**")
        st.markdown(f"- **{center}**")

        st.markdown("**Ø¨Ø¹Ø¯:**" if after else "**Ø¨Ø¹Ø¯:** Ù„Ø§ ÙŠÙˆØ¬Ø¯")
        for a in after:
            st.markdown(f"- {a}")

        st.divider()

        ctx_rows.append({
            "root": rq_norm,
            "surah_no": hit[COL_SURAH_NO],
            "ayah_no": hit[COL_AYAH_NO],
            "surah": sur_title,
            "prev_n": prev_n,
            "next_n": next_n,
            "before": " | ".join(before),
            "center": center,
            "after": " | ".join(after),
        })

    if ctx_rows:
        export_df = pd.DataFrame(ctx_rows)
        st.download_button(
            "â¬‡ï¸ ØªÙ†Ø²ÙŠÙ„ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø³ÙŠØ§Ù‚ CSV",
            data=export_df.to_csv(index=False).encode("utf-8-sig"),
            file_name=f"context_root_{rq_norm}_p{prev_n}_n{next_n}.csv",
            mime="text/csv",
        )
