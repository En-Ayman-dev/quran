# # app.py
# # -*- coding: utf-8 -*-

# import re
# import pandas as pd
# import streamlit as st

# # =======================
# # ØªØ·Ø¨ÙŠØ¹/ØªÙ†Ø¸ÙŠÙ Ø¹Ø±Ø¨ÙŠ
# # =======================
# ARABIC_DIACRITICS_RE = re.compile(r"[\u0610-\u061A\u064B-\u065F\u0670\u06D6-\u06ED]")

# def normalize_arabic(text: str) -> str:
#     """ØªØ·Ø¨ÙŠØ¹ Ø¹Ø±Ø¨ÙŠ (Ø¥Ø²Ø§Ù„Ø© ØªØ´ÙƒÙŠÙ„ + ØªÙˆØ­ÙŠØ¯ Ø¨Ø¹Ø¶ Ø§Ù„Ø­Ø±ÙˆÙ) Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ø®ØªÙ„Ø§ÙØ§Øª Ø§Ù„ÙƒØªØ§Ø¨Ø©."""
#     if not isinstance(text, str):
#         return ""
#     t = ARABIC_DIACRITICS_RE.sub("", text)
#     t = t.replace("Ø£", "Ø§").replace("Ø¥", "Ø§").replace("Ø¢", "Ø§")
#     t = t.replace("Ù±", "Ø§")
#     t = t.replace("Ø©", "Ù‡")
#     t = t.replace("Ù‰", "ÙŠ")
#     t = t.replace("Ø¤", "Ùˆ").replace("Ø¦", "ÙŠ")
#     t = re.sub(r"\s+", " ", t).strip()
#     return t

# TOKEN_RE = re.compile(r"[^\s]+")

# def tokenize(text: str):
#     if not isinstance(text, str) or not text:
#         return []
#     return TOKEN_RE.findall(text)

# # =======================
# # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# # =======================
# @st.cache_data(show_spinner=False)
# def load_quran_csv(path: str) -> pd.DataFrame:
#     try:
#         return pd.read_csv(path, dtype=str, encoding="utf-8")
#     except UnicodeDecodeError:
#         return pd.read_csv(path, dtype=str, encoding="utf-8-sig")

# @st.cache_data(show_spinner=False)
# def load_lexicon_xlsx(path: str) -> pd.DataFrame:
#     return pd.read_excel(path, dtype=str)  # ÙŠØ­ØªØ§Ø¬ openpyxl

# @st.cache_data(show_spinner=False)
# def build_root_maps(lex_df: pd.DataFrame, word_col: str, root_col: str):
#     """
#     Ù‚Ø§Ù…ÙˆØ³Ø§Ù†:
#     1) exact_map: Ù…Ø·Ø§Ø¨Ù‚Ø© Ø­Ø±ÙÙŠØ© Ù„Ù„ÙƒÙ„Ù…Ø© (ÙƒÙ…Ø§ Ù‡ÙŠ Ø¨Ø§Ù„ØªØ´ÙƒÙŠÙ„)
#     2) norm_map : Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¨Ø¹Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠØ¹/Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
#     """
#     exact_map = {}
#     norm_map = {}

#     df = lex_df[[word_col, root_col]].dropna().copy()
#     df[word_col] = df[word_col].astype(str).str.strip()
#     df[root_col] = df[root_col].astype(str).str.strip()

#     for w, r in zip(df[word_col], df[root_col]):
#         if not w or not r:
#             continue
#         exact_map[w] = r
#         nw = normalize_arabic(w)
#         if nw and nw not in norm_map:
#             norm_map[nw] = r

#     return exact_map, norm_map

# @st.cache_data(show_spinner=True)
# def index_ayah_roots(df: pd.DataFrame, col_tash: str, col_plain: str, exact_map: dict, norm_map: dict):
#     """
#     ÙŠØ¨Ù†ÙŠ:
#     - roots_set: Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¬Ø°ÙˆØ± Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„ÙŠÙ‡Ø§ ÙÙŠ Ø§Ù„Ø¢ÙŠØ©
#     - coverage: Ù†Ø³Ø¨Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙŠ ÙˆÙØ¬Ø¯ Ù„Ù‡Ø§ Ø¬Ø°Ø± (ØªÙ‚Ø¯ÙŠØ±)
#     """
#     roots_sets = []
#     coverages = []

#     for _, row in df.iterrows():
#         text_t = str(row.get(col_tash, "") or "")
#         text_p = str(row.get(col_plain, "") or "")

#         tokens_t = tokenize(text_t)
#         tokens_p = tokenize(text_p)

#         roots = set()
#         known = 0
#         total = 0

#         # 1) Ù…Ù† Ø§Ù„Ù†Øµ Ø¨Ø§Ù„ØªØ´ÙƒÙŠÙ„: Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¯Ù‚ÙŠÙ‚Ø© Ø«Ù… Ù…Ø·Ø¨Ù‘Ø¹Ø©
#         for tok in tokens_t:
#             tok = tok.strip()
#             if not tok:
#                 continue
#             total += 1

#             if tok in exact_map:
#                 roots.add(exact_map[tok])
#                 known += 1
#                 continue

#             ntok = normalize_arabic(tok)
#             if ntok in norm_map:
#                 roots.add(norm_map[ntok])
#                 known += 1
#                 continue

#         # 2) ØªØ¹Ø²ÙŠØ² Ù…Ù† Ø§Ù„Ù†Øµ Ø¨Ø¯ÙˆÙ† ØªØ´ÙƒÙŠÙ„ (Ù„Ø§ Ù†Ø²ÙŠØ¯ total)
#         for tok in tokens_p:
#             tok = tok.strip()
#             if not tok:
#                 continue
#             ntok = normalize_arabic(tok)
#             if ntok in norm_map:
#                 roots.add(norm_map[ntok])

#         coverage = (known / total) if total else 0.0
#         roots_sets.append(roots)
#         coverages.append(coverage)

#     return roots_sets, coverages

# def safe_int(x):
#     try:
#         return int(str(x))
#     except Exception:
#         return 10**9

# # =======================
# # ÙˆØ§Ø¬Ù‡Ø© Streamlit
# # =======================
# st.set_page_config(page_title="Ø¨Ø­Ø« Ø¨Ø§Ù„Ø¬Ø°Ø± (Cl1 + CSV)", page_icon="ğŸ“–", layout="wide")

# # CSS Ø§Ø­ØªØ±Ø§ÙÙŠ: ÙŠØ¯Ø¹Ù… dark/light ÙˆÙŠØ¶Ù…Ù† ØªØ¨Ø§ÙŠÙ† Ø§Ù„Ù†Øµ Ù…Ø¹ Ø§Ù„Ø¨Ø·Ø§Ù‚Ø§Øª
# st.markdown("""
# <style>
# :root { --card-radius: 16px; }

# .qcard{
#   border-radius: var(--card-radius);
#   padding: 16px 16px 12px 16px;
#   margin: 12px 0;
#   border: 1px solid rgba(0,0,0,0.08);
#   box-shadow: 0 6px 18px rgba(0,0,0,0.06);
#   direction: rtl;
# }

# @media (prefers-color-scheme: light) {
#   .qcard{ background: #ffffff; color: #111111; border-color: rgba(0,0,0,0.08); }
#   .qmuted{ color: rgba(0,0,0,0.60); }
#   .qmeta{ color: rgba(0,0,0,0.70); }
# }

# @media (prefers-color-scheme: dark) {
#   .qcard{
#     background: #151515;
#     color: #f2f2f2;
#     border-color: rgba(255,255,255,0.14);
#     box-shadow: 0 6px 18px rgba(0,0,0,0.30);
#   }
#   .qmuted{ color: rgba(255,255,255,0.70); }
#   .qmeta{ color: rgba(255,255,255,0.75); }
# }

# .qhead{ font-weight: 750; font-size: 16px; margin-bottom: 10px; }
# .qayah{ font-size: 22px; line-height: 1.9; margin: 0; }
# .qsp{ height: 10px; }
# .qbadge{
#   display: inline-block;
#   padding: 4px 10px;
#   border-radius: 999px;
#   font-size: 12px;
#   margin-top: 10px;
#   border: 1px solid rgba(127,127,127,0.25);
# }
# </style>
# """, unsafe_allow_html=True)

# st.title("ğŸ“– Ø¨Ø­Ø« Ø§Ù„Ø¢ÙŠØ§Øª Ø¨Ø§Ù„Ø¬Ø°Ø± (Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Cl1.xlsx)")

# with st.sidebar:
#     st.header("Ø§Ù„Ù…Ù„ÙØ§Øª")
#     csv_path = st.text_input("Ù…Ù„Ù Ø§Ù„Ù‚Ø±Ø¢Ù† CSV", value="quran_corrected_global.csv")
#     xlsx_path = st.text_input("Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙˆØ§Ù„Ø¬Ø°ÙˆØ± (Cl1.xlsx)", value="Cl1.xlsx")

# # ØªØ­Ù…ÙŠÙ„ CSV Ù„Ø¹Ø±Ø¶ Ø£Ø¹Ù…Ø¯ØªÙ‡ ÙƒÙ‚ÙˆØ§Ø¦Ù…
# try:
#     quran_df_raw = load_quran_csv(csv_path)
# except Exception as e:
#     st.error(f"ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ CSV: {e}")
#     st.stop()

# csv_cols = list(quran_df_raw.columns)

# with st.sidebar:
#     st.subheader("ØªØ¹ÙŠÙŠÙ† Ø£Ø¹Ù…Ø¯Ø© CSV (Ø§Ø®ØªØ± Ù…Ù† Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©)")
#     def default_idx(name, fallback=0):
#         return csv_cols.index(name) if name in csv_cols else fallback

#     col_tash = st.selectbox("Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø¢ÙŠØ© Ø¨Ø§Ù„ØªØ´ÙƒÙŠÙ„", csv_cols, index=default_idx("3", min(3, len(csv_cols)-1)))
#     col_plain = st.selectbox("Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø¢ÙŠØ© Ø¨Ø¯ÙˆÙ† ØªØ´ÙƒÙŠÙ„", csv_cols, index=default_idx("4", min(4, len(csv_cols)-1)))
#     col_surah_no = st.selectbox("Ø¹Ù…ÙˆØ¯ Ø±Ù‚Ù… Ø§Ù„Ø³ÙˆØ±Ø©", csv_cols, index=default_idx("1", min(1, len(csv_cols)-1)))
#     col_ayah_no  = st.selectbox("Ø¹Ù…ÙˆØ¯ Ø±Ù‚Ù… Ø§Ù„Ø¢ÙŠØ©", csv_cols, index=default_idx("2", min(2, len(csv_cols)-1)))
#     col_surah_t  = st.selectbox("Ø¹Ù…ÙˆØ¯ Ø§Ø³Ù… Ø§Ù„Ø³ÙˆØ±Ø© (Ø¨Ø§Ù„ØªØ´ÙƒÙŠÙ„)", csv_cols, index=default_idx("10", min(10, len(csv_cols)-1)))
#     col_surah_p  = st.selectbox("Ø¹Ù…ÙˆØ¯ Ø§Ø³Ù… Ø§Ù„Ø³ÙˆØ±Ø© (Ø¨Ø¯ÙˆÙ† ØªØ´ÙƒÙŠÙ„)", csv_cols, index=default_idx("11", min(11, len(csv_cols)-1)))

# # ØªØ­Ù…ÙŠÙ„ XLSX
# try:
#     lex_df_raw = load_lexicon_xlsx(xlsx_path)
# except Exception as e:
#     st.error(f"ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Cl1.xlsx: {e}")
#     st.stop()

# xlsx_cols = list(lex_df_raw.columns)

# with st.sidebar:
#     st.subheader("ØªØ¹ÙŠÙŠÙ† Ø£Ø¹Ù…Ø¯Ø© Cl1.xlsx")
#     word_col = st.selectbox("Ø¹Ù…ÙˆØ¯ Ø§Ù„ÙƒÙ„Ù…Ø©", xlsx_cols, index=(xlsx_cols.index("Ø§Ù„ÙƒÙ„Ù…Ø©") if "Ø§Ù„ÙƒÙ„Ù…Ø©" in xlsx_cols else 0))
#     root_col = st.selectbox("Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø¬Ø°Ø±", xlsx_cols, index=(xlsx_cols.index("Ø§Ù„Ø¬Ø°Ø±") if "Ø§Ù„Ø¬Ø°Ø±" in xlsx_cols else min(1, len(xlsx_cols)-1)))

#     st.divider()
#     display_mode = st.radio("Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬", ["Ø¨Ø§Ù„ØªØ´ÙƒÙŠÙ„", "Ø¨Ø¯ÙˆÙ† ØªØ´ÙƒÙŠÙ„", "ÙƒÙ„Ø§Ù‡Ù…Ø§"], index=2)
#     page_size = st.slider("Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ø§Ù„ØµÙØ­Ø©", 10, 200, 50, step=10)

# # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù‚ÙˆØ§Ù…ÙŠØ³
# try:
#     exact_map, norm_map = build_root_maps(lex_df_raw, word_col, root_col)
# except Exception as e:
#     st.error(f"ÙØ´Ù„ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ù…Ù† Cl1.xlsx: {e}")
#     st.stop()

# # st.caption(f"Ø§Ù„Ù‚Ø§Ù…ÙˆØ³: Ù…Ø·Ø§Ø¨Ù‚Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø©={len(exact_map):,} | Ù…Ø·Ø§Ø¨Ù‚Ø§Øª Ø¨Ø¹Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠØ¹={len(norm_map):,}")

# # ÙÙ‡Ø±Ø³Ø© Ø§Ù„Ø¬Ø°ÙˆØ± Ù„ÙƒÙ„ Ø¢ÙŠØ©
# quran_df = quran_df_raw.copy()
# roots_sets, coverages = index_ayah_roots(quran_df, col_tash, col_plain, exact_map, norm_map)
# quran_df["_roots_set"] = roots_sets
# quran_df["_coverage"] = coverages

# avg_cov = float(quran_df["_coverage"].mean())
# # st.info(f"Ù…ØªÙˆØ³Ø· ØªØºØ·ÙŠØ© Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ (ØªÙ‚Ø¯ÙŠØ±ÙŠ): {avg_cov:.1%} â€” Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…Ù†Ø®ÙØ¶Ø© ÙØ§Ù„Ù‚Ø§Ù…ÙˆØ³ Ù„Ø§ ÙŠØºØ·ÙŠ ÙƒÙ„Ù…Ø§Øª CSV Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ Ø£Ùˆ ØªÙˆØ¬Ø¯ Ø§Ø®ØªÙ„Ø§ÙØ§Øª ÙƒØªØ§Ø¨Ø©.")

# # Ø§Ù„Ø¨Ø­Ø«
# c1, c2, c3, c4 = st.columns([2.2, 1, 1, 1])
# with c1:
#     root_query = st.text_input("Ø£Ø¯Ø®Ù„ Ø§Ù„Ø¬Ø°Ø± Ù„Ù„Ø¨Ø­Ø«", placeholder="Ù…Ø«Ø§Ù„: Ø±Ø­Ù…")
# with c2:
#     surah_filter = st.text_input("Ø±Ù‚Ù… Ø§Ù„Ø³ÙˆØ±Ø© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)", placeholder="Ù…Ø«Ø§Ù„: 1")
# with c3:
#     ayah_filter = st.text_input("Ø±Ù‚Ù… Ø§Ù„Ø¢ÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)", placeholder="Ù…Ø«Ø§Ù„: 7")
# with c4:
#     run = st.button("ğŸ” Ø¨Ø­Ø«", type="primary", use_container_width=True)

# if run or root_query.strip():
#     rq = root_query.strip()
#     if not rq:
#         st.stop()

#     rq_norm = normalize_arabic(rq)

#     def has_root(rootset):
#         return rq_norm in {normalize_arabic(x) for x in (rootset or set())}

#     hits = quran_df[quran_df["_roots_set"].apply(has_root)].copy()

#     if surah_filter.strip():
#         hits = hits[hits[col_surah_no].astype(str) == surah_filter.strip()]
#     if ayah_filter.strip():
#         hits = hits[hits[col_ayah_no].astype(str) == ayah_filter.strip()]

#     total = len(hits)
#     st.subheader("Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
#     st.write(f"Ø¹Ø¯Ø¯ Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„ØªÙŠ Ø¸Ù‡Ø± ÙÙŠÙ‡Ø§ Ø§Ù„Ø¬Ø°Ø± **{rq}**: **{total}**")

#     if total == 0:
#         st.stop()

#     hits["_s"] = hits[col_surah_no].map(safe_int)
#     hits["_a"] = hits[col_ayah_no].map(safe_int)
#     hits = hits.sort_values(["_s", "_a"]).drop(columns=["_s", "_a"], errors="ignore")

#     pages = (total + page_size - 1) // page_size
#     page = st.number_input("Ø§Ù„ØµÙØ­Ø©", min_value=1, max_value=pages, value=1, step=1)
#     start = (page - 1) * page_size
#     end = min(start + page_size, total)
#     view = hits.iloc[start:end]

#     # ======== Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù‡Ù†Ø§: Ø§Ù„Ø¹Ø±Ø¶ ÙŠÙ†Ø·Ø¨Ù‚ Ø¹Ù„Ù‰ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø³ÙˆØ± Ø£ÙŠØ¶Ù‹Ø§ ========
#     def format_surah_title(sur_t: str, sur_p: str) -> str:
#         sur_t = str(sur_t or "")
#         sur_p = str(sur_p or "")
#         if display_mode == "Ø¨Ø§Ù„ØªØ´ÙƒÙŠÙ„":
#             return sur_t
#         if display_mode == "Ø¨Ø¯ÙˆÙ† ØªØ´ÙƒÙŠÙ„":
#             return sur_p
#         # ÙƒÙ„Ø§Ù‡Ù…Ø§
#         return f"{sur_t} / {sur_p}"

#     for _, row in view.iterrows():
#         sur_no = row.get(col_surah_no, "")
#         ay_no  = row.get(col_ayah_no, "")

#         sur_t  = row.get(col_surah_t, "")
#         sur_p  = row.get(col_surah_p, "")

#         ay_t   = str(row.get(col_tash, "") or "")
#         ay_p   = str(row.get(col_plain, "") or "")

#         cov = float(row.get("_coverage", 0.0)) * 100.0

#         sur_title = format_surah_title(sur_t, sur_p)

#         header_html = f"<div class='qhead'>[{sur_no}:{ay_no}] {sur_title}</div>"

#         if display_mode == "Ø¨Ø§Ù„ØªØ´ÙƒÙŠÙ„":
#             body_html = f"<p class='qayah'>{ay_t}</p>"
#         elif display_mode == "Ø¨Ø¯ÙˆÙ† ØªØ´ÙƒÙŠÙ„":
#             body_html = f"<p class='qayah'>{ay_p}</p>"
#         else:
#             body_html = (
#                 "<div class='qmuted' style='font-size:13px;margin-bottom:4px;'>Ø¨Ø§Ù„ØªØ´ÙƒÙŠÙ„</div>"
#                 f"<p class='qayah'>{ay_t}</p>"
#                 "<div class='qsp'></div>"
#                 "<div class='qmuted' style='font-size:13px;margin-bottom:4px;'>Ø¨Ø¯ÙˆÙ† ØªØ´ÙƒÙŠÙ„</div>"
#                 f"<p class='qayah'>{ay_p}</p>"
#             )

#         card_html = f"""
#         <div class="qcard">
#           {header_html}
#           {body_html}
#           <div class="qbadge qmeta">ØªØºØ·ÙŠØ© Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¢ÙŠØ© (ØªÙ‚Ø¯ÙŠØ±ÙŠ): {cov:.1f}%</div>
#         </div>
#         """

#         st.markdown(card_html, unsafe_allow_html=True)

#     st.caption(f"Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ {start+1} Ø¥Ù„Ù‰ {end} Ù…Ù† {total} â€” ØµÙØ­Ø© {page} Ù…Ù† {pages}")

# # ======================
# # ØªØµØ¯ÙŠØ± CSV Ø­Ø³Ø¨ ÙˆØ¶Ø¹ Ø§Ù„Ø¹Ø±Ø¶
# # ======================
#     st.divider()

#     base_cols = [col_surah_no, col_ayah_no]
#     export_df = hits[base_cols].copy()
#     export_df.columns = ["surah_no", "ayah_no"]

#     if display_mode == "Ø¨Ø§Ù„ØªØ´ÙƒÙŠÙ„":
#         export_df["surah"] = hits[col_surah_t]
#         export_df["ayah"]  = hits[col_tash]

#     elif display_mode == "Ø¨Ø¯ÙˆÙ† ØªØ´ÙƒÙŠÙ„":
#         export_df["surah"] = hits[col_surah_p]
#         export_df["ayah"]  = hits[col_plain]

#     else:  # ÙƒÙ„Ø§Ù‡Ù…Ø§
#         export_df["surah_tashkeel"] = hits[col_surah_t]
#         export_df["surah_plain"]    = hits[col_surah_p]
#         export_df["ayah_tashkeel"]  = hits[col_tash]
#         export_df["ayah_plain"]     = hits[col_plain]

#     st.download_button(
#         "â¬‡ï¸ ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ CSV",
#         data=export_df.to_csv(index=False).encode("utf-8-sig"),
#         file_name=f"results_root_{rq_norm}_{display_mode}.csv",
#         mime="text/csv",
#     )
# app.py
# -*- coding: utf-8 -*-

import re
import pandas as pd
import streamlit as st

# =======================
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø«Ø§Ø¨ØªØ© (Ù…Ø®ÙÙŠØ© Ø¹Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…)
# ØºÙŠÙ‘Ø±Ù‡Ø§ Ù‡Ù†Ø§ ÙÙ‚Ø· Ø¥Ø°Ø§ ØªØºÙŠÙ‘Ø± CSV Ø£Ùˆ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
# =======================
CSV_PATH = "quran_corrected_global.csv"
CL1_PATH = "Cl1.xlsx"
GROUPED_PATH = "Cl1_grouped_by_root.xlsx"

# Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©/Ø§Ù„ÙÙ‡Ø§Ø±Ø³ Ø¯Ø§Ø®Ù„ CSV (Ø­Ø³Ø¨ Ù…Ù„ÙÙƒ)
COL_TASH = "3"       # Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø¢ÙŠØ© Ø¨Ø§Ù„ØªØ´ÙƒÙŠÙ„
COL_PLAIN = "4"      # Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø¢ÙŠØ© Ø¨Ø¯ÙˆÙ† ØªØ´ÙƒÙŠÙ„
COL_SURAH_NO = "1"   # Ø±Ù‚Ù… Ø§Ù„Ø³ÙˆØ±Ø©
COL_AYAH_NO = "2"    # Ø±Ù‚Ù… Ø§Ù„Ø¢ÙŠØ©
COL_SURAH_T = "10"   # Ø§Ø³Ù… Ø§Ù„Ø³ÙˆØ±Ø© (Ø¨Ø§Ù„ØªØ´ÙƒÙŠÙ„)
COL_SURAH_P = "11"   # Ø§Ø³Ù… Ø§Ù„Ø³ÙˆØ±Ø© (Ø¨Ø¯ÙˆÙ† ØªØ´ÙƒÙŠÙ„)

# Ø£Ø¹Ù…Ø¯Ø© Cl1.xlsx
CL1_WORD_COL = "Ø§Ù„ÙƒÙ„Ù…Ø©"
CL1_ROOT_COL = "Ø§Ù„Ø¬Ø°Ø±"

# Ø£Ø¹Ù…Ø¯Ø© Cl1_grouped_by_root.xlsx
G_ROOT_COL = "Ø§Ù„Ø¬Ø°Ø±"
G_WORD_COL = "Ø§Ù„ÙƒÙ„Ù…Ø©"
G_COUNT_COL = "Ø¹Ø¯Ø¯_Ø°ÙƒØ±_Ø§Ù„ÙƒÙ„Ù…Ø©"


# =======================
# ØªØ·Ø¨ÙŠØ¹/ØªÙ†Ø¸ÙŠÙ Ø¹Ø±Ø¨ÙŠ
# =======================
ARABIC_DIACRITICS_RE = re.compile(r"[\u0610-\u061A\u064B-\u065F\u0670\u06D6-\u06ED]")
TOKEN_RE = re.compile(r"[^\s]+")

def normalize_arabic(text: str) -> str:
    """ØªØ·Ø¨ÙŠØ¹ Ø¹Ø±Ø¨ÙŠ (Ø¥Ø²Ø§Ù„Ø© ØªØ´ÙƒÙŠÙ„ + ØªÙˆØ­ÙŠØ¯ Ø¨Ø¹Ø¶ Ø§Ù„Ø­Ø±ÙˆÙ) Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ø®ØªÙ„Ø§ÙØ§Øª Ø§Ù„ÙƒØªØ§Ø¨Ø©."""
    if not isinstance(text, str):
        return ""
    t = ARABIC_DIACRITICS_RE.sub("", text)
    t = t.replace("Ø£", "Ø§").replace("Ø¥", "Ø§").replace("Ø¢", "Ø§")
    t = t.replace("Ù±", "Ø§")
    t = t.replace("Ø©", "Ù‡")
    t = t.replace("Ù‰", "ÙŠ")
    t = t.replace("Ø¤", "Ùˆ").replace("Ø¦", "ÙŠ")
    t = re.sub(r"\s+", " ", t).strip()
    return t

def tokenize(text: str):
    if not isinstance(text, str) or not text:
        return []
    return TOKEN_RE.findall(text)

def safe_int(x):
    try:
        return int(str(x))
    except Exception:
        return 10**9


# =======================
# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# =======================
@st.cache_data(show_spinner=False)
def load_quran_csv(path: str) -> pd.DataFrame:
    try:
        return pd.read_csv(path, dtype=str, encoding="utf-8")
    except UnicodeDecodeError:
        return pd.read_csv(path, dtype=str, encoding="utf-8-sig")

@st.cache_data(show_spinner=False)
def load_xlsx(path: str) -> pd.DataFrame:
    return pd.read_excel(path, dtype=str)  # ÙŠØ­ØªØ§Ø¬ openpyxl

@st.cache_data(show_spinner=False)
def build_root_maps(lex_df: pd.DataFrame, word_col: str, root_col: str):
    """
    Ù‚Ø§Ù…ÙˆØ³Ø§Ù†:
    1) exact_map: Ù…Ø·Ø§Ø¨Ù‚Ø© Ø­Ø±ÙÙŠØ© Ù„Ù„ÙƒÙ„Ù…Ø© (ÙƒÙ…Ø§ Ù‡ÙŠ Ø¨Ø§Ù„ØªØ´ÙƒÙŠÙ„)
    2) norm_map : Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¨Ø¹Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠØ¹/Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
    """
    exact_map = {}
    norm_map = {}

    df = lex_df[[word_col, root_col]].dropna().copy()
    df[word_col] = df[word_col].astype(str).str.strip()
    df[root_col] = df[root_col].astype(str).str.strip()

    for w, r in zip(df[word_col], df[root_col]):
        if not w or not r:
            continue
        exact_map[w] = r
        nw = normalize_arabic(w)
        if nw and nw not in norm_map:
            norm_map[nw] = r

    return exact_map, norm_map

@st.cache_data(show_spinner=True)
def index_ayah_roots(df: pd.DataFrame, col_tash: str, col_plain: str, exact_map: dict, norm_map: dict):
    roots_sets = []
    coverages = []

    for _, row in df.iterrows():
        text_t = str(row.get(col_tash, "") or "")
        text_p = str(row.get(col_plain, "") or "")

        tokens_t = tokenize(text_t)
        tokens_p = tokenize(text_p)

        roots = set()
        known = 0
        total = 0

        # 1) Ù…Ù† Ø§Ù„Ù†Øµ Ø¨Ø§Ù„ØªØ´ÙƒÙŠÙ„: Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¯Ù‚ÙŠÙ‚Ø© Ø«Ù… Ù…Ø·Ø¨Ù‘Ø¹Ø©
        for tok in tokens_t:
            tok = tok.strip()
            if not tok:
                continue
            total += 1

            if tok in exact_map:
                roots.add(exact_map[tok])
                known += 1
                continue

            ntok = normalize_arabic(tok)
            if ntok in norm_map:
                roots.add(norm_map[ntok])
                known += 1
                continue

        # 2) ØªØ¹Ø²ÙŠØ² Ù…Ù† Ø§Ù„Ù†Øµ Ø¨Ø¯ÙˆÙ† ØªØ´ÙƒÙŠÙ„ (Ù„Ø§ Ù†Ø²ÙŠØ¯ total)
        for tok in tokens_p:
            tok = tok.strip()
            if not tok:
                continue
            ntok = normalize_arabic(tok)
            if ntok in norm_map:
                roots.add(norm_map[ntok])

        coverage = (known / total) if total else 0.0
        roots_sets.append(roots)
        coverages.append(coverage)

    return roots_sets, coverages


# =======================
# ÙˆØ§Ø¬Ù‡Ø© Streamlit
# =======================
st.set_page_config(page_title="Ø¨Ø­Ø« Ø¨Ø§Ù„Ø¬Ø°Ø±", page_icon="ğŸ“–", layout="wide")

# CSS Ø§Ø­ØªØ±Ø§ÙÙŠ: ÙŠØ¯Ø¹Ù… dark/light ÙˆÙŠØ¶Ù…Ù† ØªØ¨Ø§ÙŠÙ† Ø§Ù„Ù†Øµ Ù…Ø¹ Ø§Ù„Ø¨Ø·Ø§Ù‚Ø§Øª
st.markdown("""
<style>
:root { --card-radius: 16px; }

.qcard{
  border-radius: var(--card-radius);
  padding: 16px 16px 12px 16px;
  margin: 12px 0;
  border: 1px solid rgba(0,0,0,0.08);
  box-shadow: 0 6px 18px rgba(0,0,0,0.06);
  direction: rtl;
}

@media (prefers-color-scheme: light) {
  .qcard{ background: #ffffff; color: #111111; border-color: rgba(0,0,0,0.08); }
  .qmuted{ color: rgba(0,0,0,0.60); }
  .qmeta{ color: rgba(0,0,0,0.70); }
}

@media (prefers-color-scheme: dark) {
  .qcard{
    background: #151515;
    color: #f2f2f2;
    border-color: rgba(255,255,255,0.14);
    box-shadow: 0 6px 18px rgba(0,0,0,0.30);
  }
  .qmuted{ color: rgba(255,255,255,0.70); }
  .qmeta{ color: rgba(255,255,255,0.75); }
}

.qhead{ font-weight: 750; font-size: 16px; margin-bottom: 10px; }
.qayah{ font-size: 22px; line-height: 1.9; margin: 0; }
.qsp{ height: 10px; }
.qbadge{
  display: inline-block;
  padding: 4px 10px;
  border-radius: 999px;
  font-size: 12px;
  margin-top: 10px;
  border: 1px solid rgba(127,127,127,0.25);
}
</style>
""", unsafe_allow_html=True)

st.title("ğŸ“– Ø¨Ø­Ø« Ø§Ù„Ø¢ÙŠØ§Øª Ø¨Ø§Ù„Ø¬Ø°Ø±")

with st.sidebar:
    st.subheader("Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¹Ø±Ø¶")
    display_mode = st.radio("Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬", ["Ø¨Ø§Ù„ØªØ´ÙƒÙŠÙ„", "Ø¨Ø¯ÙˆÙ† ØªØ´ÙƒÙŠÙ„", "ÙƒÙ„Ø§Ù‡Ù…Ø§"], index=2)
    page_size = st.slider("Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ø§Ù„ØµÙØ­Ø©", 10, 200, 50, step=10)

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª (Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ø®ÙŠØ§Ø±Ø§Øª Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…)
try:
    quran_df_raw = load_quran_csv(CSV_PATH)
except Exception as e:
    st.error(f"ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù CSV: {CSV_PATH}\n\nØ§Ù„ØªÙØ§ØµÙŠÙ„: {e}")
    st.stop()

try:
    lex_df_raw = load_xlsx(CL1_PATH)
except Exception as e:
    st.error(f"ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Cl1.xlsx: {CL1_PATH}\n\nØ§Ù„ØªÙØ§ØµÙŠÙ„: {e}")
    st.stop()

# grouped Ø§Ø®ØªÙŠØ§Ø±ÙŠ (Ù„Ùˆ ÙØ´Ù„ Ù„Ø§ ÙŠÙˆÙ‚Ù Ø§Ù„ØªØ·Ø¨ÙŠÙ‚)
group_df_raw = None
try:
    group_df_raw = load_xlsx(GROUPED_PATH)
except Exception:
    group_df_raw = None

# ØªØ­Ù‚Ù‚ Ø³Ø±ÙŠØ¹ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Ø¨Ø¯ÙˆÙ† ÙˆØ§Ø¬Ù‡Ø©)
required_csv_cols = [COL_TASH, COL_PLAIN, COL_SURAH_NO, COL_AYAH_NO, COL_SURAH_T, COL_SURAH_P]
missing_csv = [c for c in required_csv_cols if c not in quran_df_raw.columns]
if missing_csv:
    st.error(f"Ù…Ù„Ù CSV Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©: {missing_csv}\n"
             f"Ø¹Ø¯Ù‘Ù„ Ø§Ù„Ø«ÙˆØ§Ø¨Øª Ø£Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„Ù (COL_...) Ù„ØªØ·Ø§Ø¨Ù‚ Ø£Ø¹Ù…Ø¯Ø© CSV Ù„Ø¯ÙŠÙƒ.")
    st.stop()

missing_cl1 = [c for c in [CL1_WORD_COL, CL1_ROOT_COL] if c not in lex_df_raw.columns]
if missing_cl1:
    st.error(f"Ù…Ù„Ù Cl1.xlsx Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©: {missing_cl1}\n"
             f"Ø¹Ø¯Ù‘Ù„ Ø§Ù„Ø«ÙˆØ§Ø¨Øª CL1_WORD_COL / CL1_ROOT_COL Ø£Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„Ù.")
    st.stop()

# Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù‚ÙˆØ§Ù…ÙŠØ³ + ÙÙ‡Ø±Ø³Ø© Ø§Ù„Ø¬Ø°ÙˆØ± Ù„ÙƒÙ„ Ø¢ÙŠØ©
exact_map, norm_map = build_root_maps(lex_df_raw, CL1_WORD_COL, CL1_ROOT_COL)

quran_df = quran_df_raw.copy()
roots_sets, coverages = index_ayah_roots(quran_df, COL_TASH, COL_PLAIN, exact_map, norm_map)
quran_df["_roots_set"] = roots_sets
quran_df["_coverage"] = coverages

# Ø§Ù„Ø¨Ø­Ø«
c1, c2, c3, c4 = st.columns([2.2, 1, 1, 1])
with c1:
    root_query = st.text_input("Ø£Ø¯Ø®Ù„ Ø§Ù„Ø¬Ø°Ø± Ù„Ù„Ø¨Ø­Ø«", placeholder="Ù…Ø«Ø§Ù„: Ø±Ø­Ù…")
with c2:
    surah_filter = st.text_input("Ø±Ù‚Ù… Ø§Ù„Ø³ÙˆØ±Ø© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)", placeholder="Ù…Ø«Ø§Ù„: 1")
with c3:
    ayah_filter = st.text_input("Ø±Ù‚Ù… Ø§Ù„Ø¢ÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)", placeholder="Ù…Ø«Ø§Ù„: 7")
with c4:
    run = st.button("ğŸ” Ø¨Ø­Ø«", type="primary", use_container_width=True)

def format_surah_title(sur_t: str, sur_p: str) -> str:
    sur_t = str(sur_t or "")
    sur_p = str(sur_p or "")
    if display_mode == "Ø¨Ø§Ù„ØªØ´ÙƒÙŠÙ„":
        return sur_t
    if display_mode == "Ø¨Ø¯ÙˆÙ† ØªØ´ÙƒÙŠÙ„":
        return sur_p
    return f"{sur_t} / {sur_p}"

if run or root_query.strip():
    rq = root_query.strip()
    if not rq:
        st.stop()

    rq_norm = normalize_arabic(rq)

    def has_root(rootset):
        return rq_norm in {normalize_arabic(x) for x in (rootset or set())}

    hits = quran_df[quran_df["_roots_set"].apply(has_root)].copy()

    if surah_filter.strip():
        hits = hits[hits[COL_SURAH_NO].astype(str) == surah_filter.strip()]
    if ayah_filter.strip():
        hits = hits[hits[COL_AYAH_NO].astype(str) == ayah_filter.strip()]

    total = len(hits)
    st.subheader("Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
    st.write(f"Ø¹Ø¯Ø¯ Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„ØªÙŠ Ø¸Ù‡Ø± ÙÙŠÙ‡Ø§ Ø§Ù„Ø¬Ø°Ø± **{rq}**: **{total}**")

    if total == 0:
        st.stop()

    # ======================
    # ØªÙ‚Ø±ÙŠØ± ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¬Ø°Ø± (Ù…Ù† grouped file)
    # ======================
    if group_df_raw is not None:
        # Ù„Ùˆ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© Ù„Ø§ Ù†Ø¹Ø±Ø¶ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        if all(c in group_df_raw.columns for c in [G_ROOT_COL, G_WORD_COL, G_COUNT_COL]):
            try:
                gdf = group_df_raw[[G_ROOT_COL, G_WORD_COL, G_COUNT_COL]].dropna().copy()
                gdf["_root_norm"] = gdf[G_ROOT_COL].astype(str).map(normalize_arabic)
                rep = gdf[gdf["_root_norm"] == rq_norm].copy()

                rep["count"] = pd.to_numeric(rep[G_COUNT_COL].astype(str).str.strip(), errors="coerce") \
                                  .fillna(0).astype(int)

                rep["word_tashkeel"] = rep[G_WORD_COL].astype(str).str.strip()
                rep["word_plain"] = rep["word_tashkeel"].map(normalize_arabic)

                if display_mode == "Ø¨Ø§Ù„ØªØ´ÙƒÙŠÙ„":
                    rep["word"] = rep["word_tashkeel"]
                elif display_mode == "Ø¨Ø¯ÙˆÙ† ØªØ´ÙƒÙŠÙ„":
                    rep["word"] = rep["word_plain"]
                else:
                    rep["word"] = rep["word_tashkeel"] 

                rep = rep[["word", "count"]].sort_values("count", ascending=False).reset_index(drop=True)

                st.markdown("### ØªÙ‚Ø±ÙŠØ± ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¬Ø°Ø±")
                st.dataframe(rep, use_container_width=True, hide_index=True)

                st.download_button(
                    "â¬‡ï¸ ØªÙ†Ø²ÙŠÙ„ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ÙƒÙ„Ù…Ø§Øª CSV",
                    data=rep.to_csv(index=False).encode("utf-8-sig"),
                    file_name=f"root_words_{rq_norm}.csv",
                    mime="text/csv",
                )
                st.divider()
            except Exception as e:
                st.warning(f"ØªØ¹Ø°Ø± Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¬Ø°Ø±: {e}")

    # ØªØ±ØªÙŠØ¨/Ø¹Ø±Ø¶ Ø§Ù„Ø¢ÙŠØ§Øª
    hits["_s"] = hits[COL_SURAH_NO].map(safe_int)
    hits["_a"] = hits[COL_AYAH_NO].map(safe_int)
    hits = hits.sort_values(["_s", "_a"]).drop(columns=["_s", "_a"], errors="ignore")

    pages = (total + page_size - 1) // page_size
    page = st.number_input("Ø§Ù„ØµÙØ­Ø©", min_value=1, max_value=pages, value=1, step=1)
    start = (page - 1) * page_size
    end = min(start + page_size, total)
    view = hits.iloc[start:end]

    for _, row in view.iterrows():
        sur_no = row.get(COL_SURAH_NO, "")
        ay_no  = row.get(COL_AYAH_NO, "")

        sur_t  = row.get(COL_SURAH_T, "")
        sur_p  = row.get(COL_SURAH_P, "")

        ay_t   = str(row.get(COL_TASH, "") or "")
        ay_p   = str(row.get(COL_PLAIN, "") or "")

        cov = float(row.get("_coverage", 0.0)) * 100.0
        sur_title = format_surah_title(sur_t, sur_p)

        header_html = f"<div class='qhead'>[{sur_no}:{ay_no}] {sur_title}</div>"

        if display_mode == "Ø¨Ø§Ù„ØªØ´ÙƒÙŠÙ„":
            body_html = f"<p class='qayah'>{ay_t}</p>"
        elif display_mode == "Ø¨Ø¯ÙˆÙ† ØªØ´ÙƒÙŠÙ„":
            body_html = f"<p class='qayah'>{ay_p}</p>"
        else:
            body_html = (
                "<div class='qmuted' style='font-size:13px;margin-bottom:4px;'>Ø¨Ø§Ù„ØªØ´ÙƒÙŠÙ„</div>"
                f"<p class='qayah'>{ay_t}</p>"
                "<div class='qsp'></div>"
                "<div class='qmuted' style='font-size:13px;margin-bottom:4px;'>Ø¨Ø¯ÙˆÙ† ØªØ´ÙƒÙŠÙ„</div>"
                f"<p class='qayah'>{ay_p}</p>"
            )

        card_html = f"""
        <div class="qcard">
          {header_html}
          {body_html}
          <div class="qbadge qmeta">ØªØºØ·ÙŠØ© Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¢ÙŠØ© (ØªÙ‚Ø¯ÙŠØ±ÙŠ): {cov:.1f}%</div>
        </div>
        """
        st.markdown(card_html, unsafe_allow_html=True)

    st.caption(f"Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ {start+1} Ø¥Ù„Ù‰ {end} Ù…Ù† {total} â€” ØµÙØ­Ø© {page} Ù…Ù† {pages}")

    # ======================
    # ØªØµØ¯ÙŠØ± CSV Ø­Ø³Ø¨ ÙˆØ¶Ø¹ Ø§Ù„Ø¹Ø±Ø¶
    # ======================
    st.divider()

    export_df = hits[[COL_SURAH_NO, COL_AYAH_NO]].copy()
    export_df.columns = ["surah_no", "ayah_no"]

    if display_mode == "Ø¨Ø§Ù„ØªØ´ÙƒÙŠÙ„":
        export_df["surah"] = hits[COL_SURAH_T]
        export_df["ayah"]  = hits[COL_TASH]
    elif display_mode == "Ø¨Ø¯ÙˆÙ† ØªØ´ÙƒÙŠÙ„":
        export_df["surah"] = hits[COL_SURAH_P]
        export_df["ayah"]  = hits[COL_PLAIN]
    else:
        export_df["surah_tashkeel"] = hits[COL_SURAH_T]
        export_df["surah_plain"]    = hits[COL_SURAH_P]
        export_df["ayah_tashkeel"]  = hits[COL_TASH]
        export_df["ayah_plain"]     = hits[COL_PLAIN]

    st.download_button(
        "â¬‡ï¸ ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ CSV",
        data=export_df.to_csv(index=False).encode("utf-8-sig"),
        file_name=f"results_root_{rq_norm}_{display_mode}.csv",
        mime="text/csv",
    )
